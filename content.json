{"meta":{"title":"有幸的个人小站","subtitle":"code is poetry","description":"这是作者为五十年后的自己留下的，感恩生活。","author":"xiaoqinghua 有幸","url":"http://xiaoqinghua.site"},"pages":[],"posts":[{"title":"成都","slug":"成都","date":"2018-07-21T08:38:55.000Z","updated":"2018-07-21T12:51:16.036Z","comments":true,"path":"成都.html","link":"","permalink":"http://xiaoqinghua.site/成都.html","excerpt":"","text":"成 都 天 府 成 都 天 府 成 都 1I love you , I love my city!","categories":[],"tags":[{"name":"City","slug":"City","permalink":"http://xiaoqinghua.site/tags/City/"}]},{"title":"hackpad技术分析","slug":"hackpad技术分析","date":"2018-05-05T05:58:53.000Z","updated":"2018-07-14T11:45:07.872Z","comments":true,"path":"hackpad技术分析.html","link":"","permalink":"http://xiaoqinghua.site/hackpad技术分析.html","excerpt":"","text":"hackpad 技术分析前言 1、hackpad 是基于 etherpad 二次开发，增加了很多产品化的功能，如\b账户系统，文档权限等。\b腾讯文档也是基于 hackpad 开发，\b腾讯文档的表格则是基于开源项目 hansontable 作为前端载体，在协作算法上对 easysync 进行了略微的修改。 2、hackpad 同样是基于 client + server模式开发，通讯协议大体相同，做了小幅修改，OT算法\b都是使用easysync，\b客户端服务端流程可以参考 etherpad 技术分析。 3、hackpad 项目是由 scala + java + js 的组合，hackpad使用了Jetty在scala上搭建的Websocket Server (在infrastructure目录下的main.scala中完成) 实现通讯，同时也使用了etherpad使用的开源 SocketIO 库，来完成基于双向通信的实时协同。 4、以下技术调研\b是在etherpad的调研基础下对hackpad表格，图片，协议等方面的梳理。 1、行 &amp; 表格换行操作hackpad光标每一次换行，客户端都会发送 CLIENT_MESSAGE ，其中负载&quot;type&quot;:&quot;caret&quot;，changeBy\b代表作者，如图： 上述协议相当于定位的作用，结合左侧的用户实时行定位\b功能如下图： 综上：标题 &quot;caret&quot;:0, 正文的第一行 &quot;caret&quot;:1 ,正文的第n行 &quot;caret&quot;:n 例: 在标题行插入一个 a，客户端会同时发送两条协议数据 1、CLIENT_MESSAGE \b负载 caret 位置信息 1234567891011&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"CLIENT_MESSAGE\", \"payload\": &#123; \"type\": \"caret\", \"caret\": 0, \"changedBy\": \"p.1\" &#125; &#125;&#125; 2、USER_CHANGES，数据结构较于 etherpad 没有变化 1234567891011121314151617&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"USER_CHANGES\", \"baseRev\": 59, \"changeset\": \"Z:1&gt;1*0+1$a\", \"apool\": &#123; \"numToAttrib\": &#123; \"0\": [ \"author\", \"p.1\" ] &#125;, \"nextNum\": 1 &#125; &#125;&#125; 插入表格 在正文第一行插入一个表格(table)，客户端会同时发送三条协议数据 1、CLIENT_MESSAGE \b负载 caret 位置信息 插入之前，光标位置 1234567891011&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"CLIENT_MESSAGE\", \"payload\": &#123; \"type\": \"caret\", \"caret\": 1, \"changedBy\": \"p.1\" &#125; &#125;&#125; 插入之后，光标位置 1234567891011&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"CLIENT_MESSAGE\", \"payload\": &#123; \"type\": \"caret\", \"caret\": 3, \"changedBy\": \"p.1\" &#125; &#125;&#125; 2、USER_CHANGES 123456789101112131415161718192021&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"USER_CHANGES\", \"baseRev\": 61, \"changeset\": \"Z:3&gt;3|1=2*0|1+1*0*1+1*0|1+1$\\n*\\n\", \"apool\": &#123; \"numToAttrib\": &#123; \"0\": [ \"author\", \"p.1\" ], \"1\": [ \"table\", \"123\" ] &#125;, \"nextNum\": 2 &#125; &#125;&#125; 根据changeset规范解析上述通信协议数据 123pack the changeset `Z:3&gt;3|1=2*0|1+1*0*1+1*0|1+1$\\n*\\n`&#123; oldLen: 3,newLen: 6,ops: '|1=2*0|1+1*0*1+1*0|1+1',charBank: '\\n*\\n' &#125; operator:12345678910&#123; opcode: '=', chars: 2, lines: 1, attribs: '' &#125;插入一个空行 \\n&#123; opcode: '+', chars: 1, lines: 1, attribs: '*0' &#125;插入一个表格 * (*代表表格，图片，嵌入的文件)(img/embed/table)&#123; opcode: '+', chars: 1, lines: 0, attribs: '*0*1' &#125;插入一个空行 \\n&#123; opcode: '+', chars: 1, lines: 1, attribs: '*0' &#125; 插入表格代码： 123456789101112131415161718var rep = ace.getRep();// don't insert tables in title linevar selStart = [rep.selStart[0], rep.selStart[1]];var selEnd = [rep.selEnd[0], rep.selEnd[1]];if (selStart[0] == 0) &#123; selStart = [1, 0]; selEnd = [1, 0]; // make sure the table is on its own line, // (and not the title line) ace.replaceRange(selStart, selStart, '\\n\\n', []);&#125; else &#123; // make sure the table is on its own line, // (and not the title line) ace.replaceRange(selStart, selEnd, '\\n\\n', []); selStart = [selStart[0] + 1, 0];&#125;ace.replaceRange(selStart, selStart, '*', [['table', '123']]);padeditor.ace.focus(); 12345678910111213141516/** * Replace a range of text with new text. * @param &#123;Array.&lt;number&gt;&#125; start The line/column of the range start. * @param &#123;Array.&lt;number&gt;&#125; end The line/column of the range end. * @param &#123;string&#125; text The new text. * @param &#123;Array&#125; attribs The new attributes for this text. * @param &#123;boolean&#125; insertsAfterSelection The new text goes after the range. */self.replaceRange = function(start, end, text, attribs, insertsAfterSelection) &#123; self.inCallStackIfNecessary('replaceRange', function() &#123; self.fastIncorp(); self.performDocumentReplaceRange(start, end, text, attribs, insertsAfterSelection); &#125;);&#125; 编辑表格数据 在表格的第一个格子坐标(0:0)位置插入一个字符b 1234567891011121314151617&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"USER_CHANGES\", \"baseRev\": 67, \"changeset\": \"Z:7&gt;0|2=3*0=1$\", \"apool\": &#123; \"numToAttrib\": &#123; \"0\": [ \"0:0\", \"b\" ] &#125;, \"nextNum\": 1 &#125; &#125;&#125; 在表格的第一个格子坐标(0:0)位置追加一个字符c 1234567891011121314151617&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"USER_CHANGES\", \"baseRev\": 68, \"changeset\": \"Z:7&gt;0|2=3*0=1$\", \"apool\": &#123; \"numToAttrib\": &#123; \"0\": [ \"0:0\", \"bc\" ] &#125;, \"nextNum\": 1 &#125; &#125;&#125; 在表格的第一个格子坐标(0:1)位置插入一个字符d 1234567891011121314151617&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"USER_CHANGES\", \"baseRev\": 69, \"changeset\": \"Z:7&gt;0|2=3*0=1$\", \"apool\": &#123; \"numToAttrib\": &#123; \"0\": [ \"0:1\", \"d\" ] &#125;, \"nextNum\": 1 &#125; &#125;&#125; 在表格的第一个格子坐标(1:0)位置插入一个字符e 1234567891011121314151617&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"USER_CHANGES\", \"baseRev\": 70, \"changeset\": \"Z:7&gt;0|2=3*0=1$\", \"apool\": &#123; \"numToAttrib\": &#123; \"0\": [ \"1:0\", \"e\" ] &#125;, \"nextNum\": 1 &#125; &#125;&#125; 综上： 插入一个表格，会直接在apool 增加一个[&#39;table&#39;, &#39;123&#39;]属性 表格有位置坐标 | (0,0) | (0,1) | | — | — | | (1,0) | (1,1) | | (2,0) | (2,1) | 替换每个坐标里的值，会在apool中新生成一个[&quot;1:0&quot;, &quot;abc&quot;] 往每个坐标里追加值，会在apool中替换已生成的坐标位置一样的新值[&quot;1:0&quot;, &quot;123&quot;] 表格里的字符是不能使用加粗，下划线，等\b附加属性的，这个功能还有待开发（\b如果基于hackpad现有的表格设计，使用apool中的属性，已经很难再设计出附加属性的添加，猜测腾讯文档应该是做了其他的优化） 上述这些\b增加，追加的方式编辑表格数据，通过对属性apool的操作，\b\b然后根据 Changeset.applyToText(cs,str)方法，从而不需要\b把表格那一个range的 text(文本数据)进行OT算法实现就可以实现文档中表格数据的协同编辑\b。 2、图片直接\b copy 图片 url123456789101112131415161718192021&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"USER_CHANGES\", \"baseRev\": 8, \"changeset\": \"Z:s&gt;15|4=q*0*1+1*0|1+1*0+13$*\\nhttps://p.upyun.com/docs/cloud/demo.jpg\", \"apool\": &#123; \"numToAttrib\": &#123; \"0\": [ \"author\", \"p.1\" ], \"1\": [ \"img\", \"https://p.upyun.com/docs/cloud/demo.jpg\" ] &#125;, \"nextNum\": 2 &#125; &#125;&#125; 直接 copy 图片 url, 是直接在charbank:{*\\nurl}\b ,*表示图片，\\n表示换一行，url表示图片链接,和 etherpad 不同在于*表示图片。 和插入表格一样，都会在\bapool里新增属性&quot;1&quot;:[&quot;img&quot;,&quot;https://p.upyun.com/docs/cloud/demo.jpg&quot;] 上传本地图片 hackpad 图片存储使用 AWS 的 S3 存储服务 12345678910111213141516171819202122232425&#123; \"type\": \"COLLABROOM\", \"data\": &#123; \"type\": \"USER_CHANGES\", \"baseRev\": 37, \"changeset\": \"Z:f&gt;1|4=e*0*1*2+1$*\", \"apool\": &#123; \"numToAttrib\": &#123; \"0\": [ \"attachmentPlaceholder\", \"attachment-1531119782428\" ], \"1\": [ \"author\", \"p.1\" ], \"2\": [ \"img\", \"/static/img/pixel.gif\" ] &#125;, \"nextNum\": 3 &#125; &#125;&#125; 1234567891011awsUser = __aws_key_id__awsPass = __aws_secret__s3Bucket = __aws_attachments_bucket__s3Region = us-east-1相关代码:editor.performDocumentReplaceRange(start, end, '*', [ ['img', '/static/img/pixel.gif'], ['attachmentPlaceholder', attachmentId]]); 上传图片的方法： 构造FormData数据，使用ajax上传到S3\b 12345678910111213141516171819202122232425262728$.ajax(&#123; xhr: function() &#123; var xhr = new window.XMLHttpRequest(); xhr.upload.addEventListener(\"progress\", function(evt) &#123; if (evt.lengthComputable) &#123; var progress = evt.loaded / evt.total; editor.callWithAce(function (ace) &#123; setAttachmentUrlProgress(attachmentId, progress); &#125;); &#125; &#125;, false); return xhr; &#125;, url: s3host, data: form, processData: false, contentType: false, type: \"POST\", success: function() &#123; setTimeout(function() &#123; editor.callWithAce(function (ace) &#123; setAttachmentUrl(attachmentId, s3host + path, path); &#125;); observer.trigger('track', ['file-attach', null, null, &#123; success: true, padId: clientVars.padId, userId: clientVars.userId, type: file.type, path: path, size: file.size, uploadSize: shrunk.size &#125;]); &#125;, 500&#125;) 3、Ace编辑器 &amp; Changeset变更集 &amp; AttribPool属性池hackpad 的项目结构： hackpad bin 构建项目和启动项目的命令 contrib cron 包含一个清理的定时任务 glue 包含一些 python 脚本 runit 包含设置容器的一些脚本 scripts 设置数据库和服务的一些脚本 testing 测试\b目录 etherpad bin 在本地启动项目的命令 data 本地的数据，包含日志和 solr 全文搜索服务器 etc 一些\b配置文件 solr solr 全文搜索服务器 src etherpad etherpad\b 产品功能代码 locals \b语言设置 static etherpad 静态代码\b包含 ace 编辑器 easy_sync 算法等\b themes \b主题文件 .ejs 模板 appjet-eth-dev.jar 编译 infrastructure 目录下的.scala 文件生成的项目 infrastructure Scala \b目录，目前不是太懂 lib 包含一个数据库连接件 1、ace 编辑器 相对于\betherpad，ace编辑器\b功能没有太大的变化，整个ace目录结构分离，同时增加了ace_table表格，ace_media\b\b，ace_caret等抽象模块。 2、Changeset 变更集 相对于\betherpad，changeset\b功能没有变化，即便是加了table，image还是和etherpad的相同。 hackpad的changeset结构上做了分离。抽离成easysync1.js，easysync2.js。 easysync1.js包含changeset自身的一些方法，包括Changeset.decodeFromString，Changeset.numberArrayToString等。 easysync2.js包含changeset的OT功能上的一些方法，包括Changeset.follow，Changeset.compose等。 3、AttribPool 属性池 相对于\betherpad，AttribPool\b则没有改变。","categories":[{"name":"Code","slug":"Code","permalink":"http://xiaoqinghua.site/categories/Code/"},{"name":"OT","slug":"Code/OT","permalink":"http://xiaoqinghua.site/categories/Code/OT/"}],"tags":[{"name":"OT","slug":"OT","permalink":"http://xiaoqinghua.site/tags/OT/"}]},{"title":"Redis基础及高级特性与性能调优","slug":"Redis基础及高级特性与性能调优","date":"2017-07-20T14:35:52.000Z","updated":"2018-08-05T06:35:23.819Z","comments":true,"path":"Redis基础及高级特性与性能调优.html","link":"","permalink":"http://xiaoqinghua.site/Redis基础及高级特性与性能调优.html","excerpt":"","text":"本文将从 Redis 的基本特性入手，通过讲述 Redis 的数据结构和主要命令对 Redis 的基本性能进行直观的介绍。之后概览 Redis 提供的高级能力，并在部署、维护、性能调优等多个方面进行更深入的介绍和指导。 本文适合使用 Redis 的普通开发人员，以及对 Redis 进行选型、架构设计和性能调优的架构设计人员。 目录 概述 Redis 的数据结构和相关常用命令 数据持久化 内存管理和数据淘汰机制 Pipelining 事务与 Scripting Redis 性能调优 主从复制与集群分片 Redis Java 客户端的选择 概述Redis 是一个开源的，基于内存的结构化数据存储媒介，可以作为数据库、缓存服务或消息服务使用。Redis 支持多种数据结构，包括字符串、哈希表、链表、集合、有序集合、位图(Bitmap)、HyperLogLogs等。Redis 具备 LRU(Least Recently Use)淘汰、事务实现、以及不同级别的硬盘持久化等能力，并且支持副本集和通过 Redis Sentinel 实现的高可用方案，同时还支持通过 Redis Cluster 实现的数据自动分片能力。 Redis 的主要功能都是基于\b单线程模型\b实现，也就是说 Redis 使用一个线程来服务所有的客户端请求，同时 Redis 采用了\b非阻塞式IO，并精细地优化各种命令\b的算法时间复杂度，这些信息意味着： Redis 是线程安全的 (因为只有一个线程)，其所有的操作都是原子的，不会因为\b并发产生数据异常 Redis 的速度非常快（因为使用的是非阻塞式IO，且大部分命令的算法时间复杂度都是O(1)） 使用高\b耗时的 \bRedis 命令是很危险的，会占用唯一的线程的大量处理时间，导致所有请求都被拖慢。（例如时间复杂度为O(N)的KEYS命令，严格禁止在生产环境中使用） Redis 的数据结构和相关常用命令KeyRedis 采用Key-Value型的基本数据结构，任何二进制序列都可以作为 Redis 的 Key 使用（例如普通的字符串或一张\b JPEG 图片）关于 Key 的一些注意事项\b: 不要使用过长的 Key。\b例如使用一个1024字节的 key \b就不是一个好主意，不仅会消耗更多的内存，还会导致查询的效率降低 Key 短到缺失了可读性也是不好的，例如&quot;u1000flw&quot;比起&quot;user:1000:followers&quot;来说，节省了寥寥的存储空间，却引发了可读性和可维护性上的麻烦 最好使用统一规范来设计 Key ，比如&quot;object-type:id:attr&quot;，以\b这一规范设计出的 Key 可能是&quot;user:1000&quot;或者&quot;comment:1234:reply-to&quot; Redis 允许的最大 Key 长度是 512MB (对 Value 的长度限制也是 512MB) StringString 是 Redis 的基础数据类型，Redis 没有 Int、Float、Boolean等数据类型的概念，所有的基本类型在 Redis 中都以 \bString 体现 与\b String 相关的常用命令： \bSET： 为一个 key 设置 \bvalue，可以配合EX/PX参数指定 key 的有效期，\b通过NX/XX参数针对 key 是否存在的情况进行分区操作，时间复杂度O(1) GET： 获取某个 key 对用的 value，时间复杂度为O(1) GETSET： 为一个 key 设置 value，并返回该 key 的原 value，时间复杂度O(1) MSET： 为多个 key 设置 value，时间复杂度为O(N) MSETNX： 同 MSET，\b\b如果指定的 key 中有任意一个已存在，则不进行任何操作，时间复杂度O(N) MGET： 获取\b\b\b多个 key 对应的 value ，时间复杂度O(N) 上文提到过，Redis 的基本数据类型只有 String，但 Redis可以把 String 作为整形或者浮点型数字来使用，主要体现在INCR、DECR类的命令上： INCR：将 key 对应的 value 值自增1，并返回自增后的值。只对可以转换为整形的 String 数据起作用。时间复杂度O(1) INCRBY： \b将 key 对应的 value 值自增指定的整形数值，并返回自增之后的值。只对可以转换为整形的 String 数据起作用。时间复杂度为O(1) DECR/DECRBY：同INCR/INCRBY，自增改为自减 INCR/DECR系列命令要求操作的 value 类型为 String，并可以转换为64位带符号的整形数字，否则会返回错误。也就是说，进行INCR/DECR系列命令的 value，\b必须在[-2^63~2^63-1]范围内 前文提到过，Redis 采用单线程模型，天然是线程安全的，这使得INCR/DECR命令可以非常便利的实现高并发场景\b下的精确控制。 例1：库存控制在高并发场景下实现\b库存余量的精准校验，确保不出现超卖的情况\b设置库存总量： 1SET inv:remain &quot;100&quot; 库存扣减 + 余量校验 1DECR inv:remain 当DECR\b命令返回值大于等于0时，说明库存余量通过校验，如果返回小于0的值，则说明库存已经耗尽。 假设同时有300个并发请求进行库存扣减，Redis 能够确保这300个请求分别得到99 ~ -200的返回值，每个请求得到的返回值都是唯一的，绝对不会出现两个请求得到一样的返回的情况。 例2：自增序列生成实现类似于RDBMS的Sequence功能，生成一系列唯一序列号 设置序列起始值： 1SET sequence &quot;10000&quot; 获取一个序列值： 1INCR sequence 直接将返回值作为序列使用即可。 获取一批（如100个）序列值： 1INCRBY sequence 100 假设返回值为N，那么[N - 99 ~ N]的数值都是可以用的序列值。 当多个客户端同时向 Redis 申请自增序列时，Redis 能够确保每个客户端得到的序列值或序列范围都是全局唯一的。绝对不会出现不同客户端得到了重复的序列值的情况。 ListRedis 的 list 是链表的数据结构，可以使用LPUSH/RPUSH/LPOP/RPOP等命令在 List 的连段执行插入元素和弹出元素的操作。虽然 List 也支持在特定index上插入和读取元素的功能，但其时间复杂度较高(O(N))，应小心使用。 与 List 相关的常用命令： LPUSH： 向指定的 List 的左侧（即头部）插入1个或多个元素，返回插入后的 List 长度。时间复杂度O(N), N 为插入元素的数量 RPUSH：同 LPUSH，向 List 的右侧（即尾部）插入一个或多个元素 LPOP：从指定的 List 的左侧（即头部）移除一个元素并返回，时间复杂度O(1) RPOP：同 LPOP ，从指定的 List 的右侧（即尾部）移除一个元素并返回 LPUSHX/RPUSHX：与 LPUSH/RPUSH 类似，区别在于，LPUSHX/RPUSHX 操作的 key 如果不存在，则不会进行任何操作 LLEN：返回指定的 List 长度，时间复杂度O(1) LRANGE：返回指定 List 中指定范围的元素 （双端包含，即 LRANGE key 0 10 会返回10个元素），时间复杂度O(N)。应尽可能控制以此获取的元素数量，一次获取过大范围的 List 元素会导致延迟，同时对长度不可预知的 List ，避免使用 LRANGE key 0 -1这样的完整便利操作 应谨慎使用的 List 相关命令： LINDEX：返回指定 List 指定 index 上的元素，如果 index 越界，返回 nil。index 数值是回环的，即 -1 代表 List 最后一个位置，-2 代表 List 倒数第二个位置。时间复杂度O(N) LSET：将指定 List 指定 index 上的元素设置为 value，如果 index 越界则返回错误，时间复杂度O(N)，如果操作的是头/尾部的元素，则时间复杂度为O(1) LINSERT：向指定 List 中指定元素之前/之后插入一个新元素，并返回操作后的 List 长度。如果指定的元素不存在，返回 -1。如果指定 key 不存在，不会进行任何操作，时间复杂度O(N) 由于 Redis 的 List 是链表结构的，上述的三个命令的算法效率较低，需要对List进行遍历，命令的耗时无法预估，在List长度大的情况下耗时会明显增加，应谨慎使用。 换句话说，Redis 的 List 实际是设计来用于实现队列，而不是用于实现类似ArrayList这样的列表的。如果你不是想要实现一个双端出入的队列，那么请尽量不要使用 Redis 的 List 数据结构。 为了更好支持队列的特性，Redis 还提供了一系列阻塞式的操作命令，如BLPOP/BRPOP等，能够实现类似于BlockingQueue的能力，即在 List 为空时，阻塞该连接，直到 List 中有对象可以出队时再返回。 HashHash即哈希表，Redis的Hash和传统的哈希表一样，是一种field-value型的数据结构，可以理解成将HashMap搬入Redis。Hash非常适合用于表现对象类型的数据，用Hash中的field对应对象的field即可。Hash的优点包括： 可以实现二元查找，如”查找ID为1000的用户的年龄” 比起将整个对象序列化后作为String存储的方法，Hash能够有效地减少网络传输的消耗 当使用Hash维护一个集合时，提供了比List效率高得多的随机访问命令 与Hash相关的常用命令： HSET：将key对应的Hash中的field设置为value。如果该Hash不存在，会自动创建一个。时间复杂度O(1) HGET：返回指定Hash中field字段的值，时间复杂度O(1) HMSET/HMGET：同HSET和HGET，可以批量操作同一个key下的多个field，时间复杂度：O(N)，N为一次操作的field数量 HSETNX：同HSET，但如field已经存在，HSETNX不会进行任何操作，时间复杂度O(1) HEXISTS：判断指定Hash中field是否存在，存在返回1，不存在返回0，时间复杂度O(1) HDEL：删除指定Hash中的field（1个或多个），时间复杂度：O(N)，N为操作的field数量 HINCRBY：同INCRBY命令，对指定Hash中的一个field进行INCRBY，时间复杂度O(1) 应谨慎使用的Hash相关命令： HGETALL：返回指定Hash中所有的field-value对。返回结果为数组，数组中field和value交替出现。时间复杂度O(N) HKEYS/HVALS：返回指定Hash中所有的field/value，时间复杂度O(N) 上述三个命令都会对Hash进行完整遍历，Hash中的field数量与命令的耗时线性相关，对于尺寸不可预知的Hash，应严格避免使用上面三个命令，而改为使用HSCAN命令进行游标式的遍历； SetRedis Set是无序的，不可重复的String集合。 与Set相关的常用命令： SADD：向指定Set中添加1个或多个member，如果指定Set不存在，会自动创建一个。时间复杂度O(N)，N为添加的member个数 SREM：从指定Set中移除1个或多个member，时间复杂度O(N)，N为移除的member个数 SRANDMEMBER：从指定Set中随机返回1个或多个member，时间复杂度O(N)，N为返回的member个数 SPOP：从指定Set中随机移除并返回count个member，时间复杂度O(N)，N为移除的member个数 SCARD：返回指定Set中的member个数，时间复杂度O(1) SISMEMBER：判断指定的value是否存在于指定Set中，时间复杂度O(1) SMOVE：将指定member从一个Set移至另一个Set 慎用的Set相关命令： SMEMBERS：返回指定Hash中所有的member，时间复杂度O(N) SUNION/SUNIONSTORE：计算多个Set的并集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数 SINTER/SINTERSTORE：计算多个Set的交集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数 SDIFF/SDIFFSTORE：计算1个Set与1或多个Set的差集并返回/存储至另一个Set中，时间复杂度O(N)，N为参与计算的所有集合的总member数 上述几个命令涉及的计算量大，应谨慎使用，特别是在参与计算的Set尺寸不可知的情况下，应严格避免使用。可以考虑通过SSCAN命令遍历获取相关Set的全部member，如果需要做并集/交集/差集计算，可以在客户端进行，或在不服务实时查询请求的Slave上进行。 Sorted SetRedis Sorted Set是有序的、不可重复的String集合。Sorted Set中的每个元素都需要指派一个分数(score)，Sorted Set会根据score对元素进行升序排序。如果多个member拥有相同的score，则以字典序进行升序排序。 Sorted Set非常适合用于实现排名。 Sorted Set的主要命令： ZADD：向指定Sorted Set中添加1个或多个member，时间复杂度O(Mlog(N))，M为添加的member数量，N为Sorted Set中的member数量 ZREM：从指定Sorted Set中删除1个或多个member，时间复杂度O(Mlog(N))，M为删除的member数量，N为Sorted Set中的member数量 ZCOUNT：返回指定Sorted Set中指定score范围内的member数量，时间复杂度：O(log(N)) ZCARD：返回指定Sorted Set中的member数量，时间复杂度O(1) ZSCORE：返回指定Sorted Set中指定member的score，时间复杂度O(1) ZRANK/ZREVRANK：返回指定member在Sorted Set中的排名，ZRANK返回按升序排序的排名，ZREVRANK则返回按降序排序的排名。时间复杂度O(log(N)) ZINCRBY：同INCRBY，对指定Sorted Set中的指定member的score进行自增，时间复杂度O(log(N)) 慎用的Sorted Set相关命令： ZRANGE/ZREVRANGE：返回指定Sorted Set中指定排名范围内的所有member，ZRANGE为按score升序排序，ZREVRANGE为按score降序排序，时间复杂度O(log(N)+M)，M为本次返回的member数 ZRANGEBYSCORE/ZREVRANGEBYSCORE：返回指定Sorted Set中指定score范围内的所有member，返回结果以升序/降序排序，min和max可以指定为-inf和+inf，代表返回所有的member。时间复杂度O(log(N)+M) ZREMRANGEBYRANK/ZREMRANGEBYSCORE：移除Sorted Set中指定排名范围/指定score范围内的所有member。时间复杂度O(log(N)+M) 上述几个命令，应尽量避免传递[0 -1]或[-inf +inf]这样的参数，来对Sorted Set做一次性的完整遍历，特别是在Sorted Set的尺寸不可预知的情况下。可以通过ZSCAN命令来进行游标式的遍历，或通过LIMIT参数来限制返回member的数量（适用于ZRANGEBYSCORE和ZREVRANGEBYSCORE命令），以实现游标式的遍历。 Bitmap和HyperLogLogRedis的这两种数据结构相较之前的并不常用，在本文中只做简要介绍，如想要详细了解这两种数据结构与其相关的命令； Bitmap在Redis中不是一种实际的数据类型，而是一种将String作为Bitmap使用的方法。可以理解为将String转换为bit数组。使用Bitmap来存储true/false类型的简单数据极为节省空间。 HyperLogLogs是一种主要用于数量统计的数据结构，它和Set类似，维护一个不可重复的String集合，但是HyperLogLogs并不维护具体的member内容，只维护member的个数。也就是说，HyperLogLogs只能用于计算一个集合中不重复的元素数量，所以它比Set要节省很多内存空间。 其他常用命令 EXISTS：判断指定的key是否存在，返回1代表存在，0代表不存在，时间复杂度O(1) DEL：删除指定的key及其对应的value，时间复杂度O(N)，N为删除的key数量 EXPIRE/PEXPIRE：为一个key设置有效期，单位为秒或毫秒，时间复杂度O(1) TTL/PTTL：返回一个key剩余的有效时间，单位为秒或毫秒，时间复杂度O(1) RENAME/RENAMENX：将key重命名为newkey。使用RENAME时，如果newkey已经存在，其值会被覆盖；使用RENAMENX时，如果newkey已经存在，则不会进行任何操作，时间复杂度O(1) TYPE：返回指定key的类型，string, list, set, zset, hash。时间复杂度O(1) CONFIG GET：获得Redis某配置项的当前值，可以使用*通配符，时间复杂度O(1) CONFIG SET：为Redis某个配置项设置新值，时间复杂度O(1) CONFIG REWRITE：让Redis重新加载redis.conf中的配置 数据持久化Redis提供了将数据定期自动持久化至硬盘的能力，包括RDB和AOF两种方案，两种方案分别有其长处和短板，可以配合起来同时运行，确保数据的稳定性。 必须使用数据持久化吗？Redis的数据持久化机制是可以关闭的。如果你只把Redis作为缓存服务使用，Redis中存储的所有数据都不是该数据的主体而仅仅是同步过来的备份，那么可以关闭Redis的数据持久化机制。但通常来说，仍然建议至少开启RDB方式的数据持久化，因为： RDB方式的持久化几乎不损耗Redis本身的性能，在进行RDB持久化时，Redis主进程唯一需要做的事情就是fork出一个子进程，所有持久化工作都由子进程完成 Redis无论因为什么原因crash掉之后，重启时能够自动恢复到上一次RDB快照中记录的数据。这省去了手工从其他数据源（如DB）同步数据的过程，而且要比其他任何的数据恢复方式都要快 现在硬盘那么大，真的不缺那一点地方 RDB采用RDB持久方式，Redis会定期保存数据快照至一个rbd文件中，并在启动时自动加载rdb文件，恢复之前保存的数据。可以在配置文件中配置Redis进行快照保存的时机： 1save [secends] [changes] 意为在[seconds]秒内如果发生了[changes]次数据修改，则进行一次RDB快照保存，例如： 1save 60 100 会让Redis每60秒检查一次数据变更情况，如果发生了100次或以上的数据变更，则进行RDB快照保存。可以配置多条save指令，让Redis执行多级的快照保存策略。Redis默认开启RDB快照，默认的RDB策略如下： 123save 900 1save 300 10save 60 10000 也可以通过BGSAVE命令手工触发RDB快照保存。 RDB的优点： 对性能影响最小。如前文所述，Redis在保存RDB快照时会fork出子进程进行，几乎不影响Redis处理客户端请求的效率。 每次快照会生成一个完整的数据快照文件，所以可以辅以其他手段保存多个时间点的快照（例如把每天0点的快照备份至其他存储媒介中），作为非常可靠的灾难恢复手段。 使用RDB文件进行数据恢复比使用AOF要快很多。 RDB的缺点： 快照是定期生成的，所以在Redis crash时或多或少会丢失一部分数据。 如果数据集非常大且CPU不够强（比如单核CPU），Redis在fork子进程时可能会消耗相对较长的时间（长至1秒），影响这期间的客户端请求。 AOF采用AOF持久方式时，Redis会把每一个写请求都记录在一个日志文件里。在Redis重启时，会把AOF文件中记录的所有写操作顺序执行一遍，确保数据恢复到最新。 AOF默认是关闭的，如要开启，进行如下配置： 1appendonly yes AOF提供了三种fsync配置，always/everysec/no，通过配置项[appendfsync]指定： appendfsync no：不进行fsync，将flush文件的时机交给OS决定，速度最快 appendfsync always：每写入一条日志就进行一次fsync操作，数据安全性最高，但速度最慢 appendfsync everysec：折中的做法，交由后台线程每秒fsync一次 随着AOF不断地记录写操作日志，必定会出现一些无用的日志，例如某个时间点执行了命令SET key1 “abc”，在之后某个时间点又执行了SET key1 “bcd”，那么第一条命令很显然是没有用的。大量的无用日志会让AOF文件过大，也会让数据恢复的时间过长。所以Redis提供了AOF rewrite功能，可以重写AOF文件，只保留能够把数据恢复到最新状态的最小写操作集。AOF rewrite可以通过BGREWRITEAOF命令触发，也可以配置Redis定期自动进行： 12auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 上面两行配置的含义是，Redis在每次AOF rewrite时，会记录完成rewrite后的AOF日志大小，当AOF日志大小在该基础上增长了100%后，自动进行AOF rewrite。同时如果增长的大小没有达到64mb，则不会进行rewrite。 AOF的优点： 最安全，在启用appendfsync always时，任何已写入的数据都不会丢失，使用在启用appendfsync everysec也至多只会丢失1秒的数据。 AOF文件在发生断电等问题时也不会损坏，即使出现了某条日志只写入了一半的情况，也可以使用redis-check-aof工具轻松修复。 AOF文件易读，可修改，在进行了某些错误的数据清除操作后，只要AOF文件没有rewrite，就可以把AOF文件备份出来，把错误的命令删除，然后恢复数据。 AOF的缺点： AOF文件通常比RDB文件更大 性能消耗比RDB高 数据恢复速度比RDB慢 内存管理与数据淘汰机制最大内存设置默认情况下，在32位OS中，Redis最大使用3GB的内存，在64位OS中则没有限制。 在使用Redis时，应该对数据占用的最大空间有一个基本准确的预估，并为Redis设定最大使用的内存。否则在64位OS中Redis会无限制地占用内存（当物理内存被占满后会使用swap空间），容易引发各种各样的问题。 通过如下配置控制Redis使用的最大内存： 1maxmemory 100mb 在内存占用达到maxmemory后，再向Redis写入数据时，Redis会： 根据配置的数据淘汰策略尝试淘汰数据，释放空间 如果没有数据可以淘汰，或者没有配置数据淘汰策略，那么Redis会对所有写请求返回错误，但读请求仍然可以正常执行 在为Redis设置maxmemory时，需要注意： 如果采用了Redis的主从同步，主节点向从节点同步数据时，会占用掉一部分内存空间，如果maxmemory过于接近主机的可用内存，导致数据同步时内存不足。所以设置的maxmemory不要过于接近主机可用的内存，留出一部分预留用作主从同步。 数据淘汰机制Redis提供了5种数据淘汰策略： volatile-lru：使用LRU算法进行数据淘汰（淘汰上次使用时间最早的，且使用次数最少的key），只淘汰设定了有效期的key allkeys-lru：使用LRU算法进行数据淘汰，所有的key都可以被淘汰 volatile-random：随机淘汰数据，只淘汰设定了有效期的key allkeys-random：随机淘汰数据，所有的key都可以被淘汰 volatile-ttl：淘汰剩余有效期最短的key 最好为Redis指定一种有效的数据淘汰策略以配合maxmemory设置，避免在内存使用满后发生写入失败的情况。 一般来说，推荐使用的策略是volatile-lru，并辨识Redis中保存的数据的重要性。对于那些重要的，绝对不能丢弃的数据（如配置类数据等），应不设置有效期，这样Redis就永远不会淘汰这些数据。对于那些相对不是那么重要的，并且能够热加载的数据（比如缓存最近登录的用户信息，当在Redis中找不到时，程序会去DB中读取），可以设置上有效期，这样在内存不够时Redis就会淘汰这部分数据。 配置方法： 1maxmemory-policy volatile-lru #默认是noeviction，即不进行数据淘汰 PipelinlingPipelinlingRedis提供许多批量操作的命令，如MSET/MGET/HMSET/HMGET等等，这些命令存在的意义是减少维护网络连接和传输数据所消耗的资源和时间。例如连续使用5次SET命令设置5个不同的key，比起使用一次MSET命令设置5个不同的key，效果是一样的，但前者会消耗更多的RTT(Round Trip Time)时长，永远应优先使用后者。 然而，如果客户端要连续执行的多次操作无法通过Redis命令组合在一起，例如： 123SET a &quot;abc&quot;INCR bHSET c name &quot;hi&quot; 此时便可以使用Redis提供的pipelining功能来实现在一次交互中执行多条命令。使用pipelining时，只需要从客户端一次向Redis发送多条命令（以\\r\\n）分隔，Redis就会依次执行这些命令，并且把每个命令的返回按顺序组装在一起一次返回，比如： 1234$ (printf &quot;PING\\r\\nPING\\r\\nPING\\r\\n&quot;; sleep 1) | nc localhost 6379+PONG+PONG+PONG 大部分的Redis客户端都对Pipelining提供支持，所以开发者通常并不需要自己手工拼装命令列表。 Pipelining的局限性Pipelining只能用于执行连续且无相关性的命令，当某个命令的生成需要依赖于前一个命令的返回时，就无法使用Pipelining了。 通过Scripting功能，可以规避这一局限性 事务与ScriptingPipelining能够让Redis在一次交互中处理多条命令，然而在一些场景下，我们可能需要在此基础上确保这一组命令是连续执行的。 比如获取当前累计的PV数并将其清0 1234&gt; GET vCount12384&gt; SET vCount 0OK 如果在GET和SET命令之间插进来一个INCR vCount，就会使客户端拿到的vCount不准确。 Redis的事务可以确保复数命令执行时的原子性。也就是说Redis能够保证：一个事务中的一组命令是绝对连续执行的，在这些命令执行完成之前，绝对不会有来自于其他连接的其他命令插进去执行。 通过MULTI和EXEC命令来把这两个命令加入一个事务中： 123456789&gt; MULTIOK&gt; GET vCountQUEUED&gt; SET vCount 0QUEUED&gt; EXEC1) 123842) OK Redis在接收到MULTI命令后便会开启一个事务，这之后的所有读写命令都会保存在队列中但并不执行，直到接收到EXEC命令后，Redis会把队列中的所有命令连续顺序执行，并以数组形式返回每个命令的返回结果。 可以使用DISCARD命令放弃当前的事务，将保存的命令队列清空。 需要注意的是，Redis事务不支持回滚：如果一个事务中的命令出现了语法错误，大部分客户端驱动会返回错误，2.6.5版本以上的Redis也会在执行EXEC时检查队列中的命令是否存在语法错误，如果存在，则会自动放弃事务并返回错误。但如果一个事务中的命令有非语法类的错误（比如对String执行HSET操作），无论客户端驱动还是Redis都无法在真正执行这条命令之前发现，所以事务中的所有命令仍然会被依次执行。在这种情况下，会出现一个事务中部分命令成功部分命令失败的情况，然而与RDBMS不同，Redis不提供事务回滚的功能，所以只能通过其他方法进行数据的回滚。 通过事务实现CASRedis提供了WATCH命令与事务搭配使用，实现CAS乐观锁的机制。 假设要实现将某个商品的状态改为已售： 12if(exec(HGET stock:1001 state) == &quot;in stock&quot;) exec(HSET stock:1001 state &quot;sold&quot;); 这一伪代码执行时，无法确保并发安全性，有可能多个客户端都获取到了”in stock”的状态，导致一个库存被售卖多次。 使用WATCH命令和事务可以解决这一问题： 123456exec(WATCH stock:1001);if(exec(HGET stock:1001 state) == &quot;in stock&quot;) &#123; exec(MULTI); exec(HSET stock:1001 state &quot;sold&quot;); exec(EXEC);&#125; WATCH的机制是：在事务EXEC命令执行时，Redis会检查被WATCH的key，只有被WATCH的key从WATCH起始时至今没有发生过变更，EXEC才会被执行。如果WATCH的key在WATCH命令到EXEC命令之间发生过变化，则EXEC命令会返回失败。 Scripting通过EVAL与EVALSHA命令，可以让Redis执行LUA脚本。这就类似于RDBMS的存储过程一样，可以把客户端与Redis之间密集的读/写交互放在服务端进行，避免过多的数据交互，提升性能。 Scripting功能是作为事务功能的替代者诞生的，事务提供的所有能力Scripting都可以做到。Redis官方推荐使用LUA Script来代替事务，前者的效率和便利性都超过了事务。 Redis性能调优尽管Redis是一个非常快速的内存数据存储媒介，也并不代表Redis不会产生性能问题。前文中提到过，Redis采用单线程模型，所有的命令都是由一个线程串行执行的，所以当某个命令执行耗时较长时，会拖慢其后的所有命令，这使得Redis对每个任务的执行效率更加敏感。 针对Redis的性能优化，主要从下面几个层面入手： 最初的也是最重要的，确保没有让Redis执行耗时长的命令 使用pipelining将连续执行的命令组合执行 操作系统的Transparent huge pages功能必须关闭： 1echo never &gt; /sys/kernel/mm/transparent_hugepage/enable 如果在虚拟机中运行Redis，可能天然就有虚拟机环境带来的固有延迟。可以通过./redis-cli –intrinsic-latency 100命令查看固有延迟。同时如果对Redis的性能有较高要求的话，应尽可能在物理机上直接部署Redis。 检查数据持久化策略 考虑引入读写分离机制 长耗时命令Redis绝大多数读写命令的时间复杂度都在O(1)到O(N)之间，在文本和官方文档中均对每个命令的时间复杂度有说明。 通常来说，O(1)的命令是安全的，O(N)命令在使用时需要注意，如果N的数量级不可预知，则应避免使用。例如对一个field数未知的Hash数据执行HGETALL/HKEYS/HVALS命令，通常来说这些命令执行的很快，但如果这个Hash中的field数量极多，耗时就会成倍增长。又如使用SUNION对两个Set执行Union操作，或使用SORT对List/Set执行排序操作等时，都应该严加注意。 避免在使用这些O(N)命令时发生问题主要有几个办法： 不要把List当做列表使用，仅当做队列来使用 通过机制严格控制Hash、Set、Sorted Set的大小 可能的话，将排序、并集、交集等操作放在客户端执行 绝对禁止使用KEYS命令 避免一次性遍历集合类型的所有成员，而应使用SCAN类的命令进行分批的，游标式的遍历 Redis提供了SCAN命令，可以对Redis中存储的所有key进行游标式的遍历，避免使用KEYS命令带来的性能问题。同时还有SSCAN/HSCAN/ZSCAN等命令，分别用于对Set/Hash/Sorted Set中的元素进行游标式遍历。 Redis提供了Slow Log功能，可以自动记录耗时较长的命令。相关的配置参数有两个： 12slowlog-log-slower-than xxxms #执行时间慢于xxx毫秒的命令计入Slow Logslowlog-max-len xxx #Slow Log的长度，即最大纪录多少条Slow Log 使用SLOWLOG GET [number]命令，可以输出最近进入Slow Log的number条命令。使用SLOWLOG RESET命令，可以重置Slow Log 网络引发的延迟 尽可能使用长连接或连接池，避免频繁创建销毁连接 客户端进行的批量数据操作，应使用Pipeline特性在一次交互中完成。具体请参照本文的Pipelining章节 数据持久化引发的延迟Redis的数据持久化工作本身就会带来延迟，需要根据数据的安全级别和性能要求制定合理的持久化策略： AOF + fsync always的设置虽然能够绝对确保数据安全，但每个操作都会触发一次fsync，会对Redis的性能有比较明显的影响 AOF + fsync every second是比较好的折中方案，每秒fsync一次 AOF + fsync never会提供AOF持久化方案下的最优性能 使用RDB持久化通常会提供比使用AOF更高的性能，但需要注意RDB的策略配置 每一次RDB快照和AOF Rewrite都需要Redis主进程进行fork操作。fork操作本身可能会产生较高的耗时，与CPU和Redis占用的内存大小有关。根据具体的情况合理配置RDB快照和AOF Rewrite时机，避免过于频繁的fork带来的延迟 1Redis在fork子进程时需要将内存分页表拷贝至子进程，以占用了24GB内存的Redis实例为例，共需要拷贝24GB / 4kB * 8 = 48MB的数据。在使用单Xeon 2.27Ghz的物理机上，这一fork操作耗时216ms。 1可以通过INFO命令返回的latest_fork_usec字段查看上一次fork操作的耗时（微秒） Swap引发的延迟当Linux将Redis所用的内存分页移至swap空间时，将会阻塞Redis进程，导致Redis出现不正常的延迟。Swap通常在物理内存不足或一些进程在进行大量I/O操作时发生，应尽可能避免上述两种情况的出现。 /proc//smaps文件中会保存进程的swap记录，通过查看这个文件，能够判断Redis的延迟是否由Swap产生。如果这个文件中记录了较大的Swap size，则说明延迟很有可能是Swap造成的。 数据淘汰引发的延迟当同一秒内有大量key过期时，也会引发Redis的延迟。在使用时应尽量将key的失效时间错开。 引入读写分离机制Redis的主从复制能力可以实现一主多从的多节点架构，在这一架构下，主节点接收所有写请求，并将数据同步给多个从节点。在这一基础上，我们可以让从节点提供对实时性要求不高的读请求服务，以减小主节点的压力。尤其是针对一些使用了长耗时命令的统计类任务，完全可以指定在一个或多个从节点上执行，避免这些长耗时命令影响其他请求的响应。 主从复制与集群分片主从复制Redis支持一主多从的主从复制架构。一个Master实例负责处理所有的写请求，Master将写操作同步至所有Slave。借助Redis的主从复制，可以实现读写分离和高可用： 实时性要求不是特别高的读请求，可以在Slave上完成，提升效率。特别是一些周期性执行的统计任务，这些任务可能需要执行一些长耗时的Redis命令，可以专门规划出1个或几个Slave用于服务这些统计任务 借助Redis Sentinel可以实现高可用，当Master crash后，Redis Sentinel能够自动将一个Slave晋升为Master，继续提供服务 启用主从复制非常简单，只需要配置多个Redis实例，在作为Slave的Redis实例中配置： 1slaveof 192.168.1.1 6379 #指定Master的IP和端口 当Slave启动后，会从Master进行一次冷启动数据同步，由Master触发BGSAVE生成RDB文件推送给Slave进行导入，导入完成后Master再将增量数据通过Redis Protocol同步给Slave。之后主从之间的数据便一直以Redis Protocol进行同步 使用Sentinel做自动failoverRedis的主从复制功能本身只是做数据同步，并不提供监控和自动failover能力，要通过主从复制功能来实现Redis的高可用，还需要引入一个组件：Redis Sentinel Redis Sentinel是Redis官方开发的监控组件，可以监控Redis实例的状态，通过Master节点自动发现Slave节点，并在监测到Master节点失效时选举出一个新的Master，并向所有Redis实例推送新的主从配置。 Redis Sentinel需要至少部署3个实例才能形成选举关系。 关键配置： 1234sentinel monitor mymaster 127.0.0.1 6379 2 #Master实例的IP、端口，以及选举需要的赞成票数sentinel down-after-milliseconds mymaster 60000 #多长时间没有响应视为Master失效sentinel failover-timeout mymaster 180000 #两次failover尝试间的间隔时长sentinel parallel-syncs mymaster 1 #如果有多个Slave，可以通过此配置指定同时从新Master进行数据同步的Slave数，避免所有Slave同时进行数据同步导致查询服务也不可用 另外需要注意的是，Redis Sentinel实现的自动failover不是在同一个IP和端口上完成的，也就是说自动failover产生的新Master提供服务的IP和端口与之前的Master是不一样的，所以要实现HA，还要求客户端必须支持Sentinel，能够与Sentinel交互获得新Master的信息才行。 集群分片为何要做集群分片： Redis中存储的数据量大，一台主机的物理内存已经无法容纳 Redis的写请求并发量大，一个Redis实例以无法承载 当上述两个问题出现时，就必须要对Redis进行分片了。Redis的分片方案有很多种，例如很多Redis的客户端都自行实现了分片功能，也有向Twemproxy这样的以代理方式实现的Redis分片方案。然而首选的方案还应该是Redis官方在3.0版本中推出的Redis Cluster分片方案。 本文不会对Redis Cluster的具体安装和部署细节进行介绍，重点介绍Redis Cluster带来的好处与弊端。 Redis Cluster的能力 能够自动将数据分散在多个节点上 当访问的key不在当前分片上时，能够自动将请求转发至正确的分片 当集群中部分节点失效时仍能提供服务 其中第三点是基于主从复制来实现的，Redis Cluster的每个数据分片都采用了主从复制的结构，原理和前文所述的主从复制完全一致，唯一的区别是省去了Redis Sentinel这一额外的组件，由Redis Cluster负责进行一个分片内部的节点监控和自动failover。 Redis Cluster分片原理Redis Cluster中共有16384个hash slot，Redis会计算每个key的CRC16，将结果与16384取模，来决定该key存储在哪一个hash slot中，同时需要指定Redis Cluster中每个数据分片负责的Slot数。Slot的分配在任何时间点都可以进行重新分配。 客户端在对key进行读写操作时，可以连接Cluster中的任意一个分片，如果操作的key不在此分片负责的Slot范围内，Redis Cluster会自动将请求重定向到正确的分片上。 hash tags在基础的分片原则上，Redis还支持hash tags功能，以hash tags要求的格式明明的key，将会确保进入同一个Slot中。例如：{uiv}user:1000和{uiv}user:1001拥有同样的hash tag {uiv}，会保存在同一个Slot中。 使用Redis Cluster时，pipelining、事务和LUA Script功能涉及的key必须在同一个数据分片上，否则将会返回错误。如要在Redis Cluster中使用上述功能，就必须通过hash tags来确保一个pipeline或一个事务中操作的所有key都位于同一个Slot中。 1有一些客户端（如Redisson）实现了集群化的pipelining操作，可以自动将一个pipeline里的命令按key所在的分片进行分组，分别发到不同的分片上执行。但是Redis不支持跨分片的事务，事务和LUA Script还是必须遵循所有key在一个分片上的规则要求。 主从复制 vs 集群分片在设计软件架构时，要如何在主从复制和集群分片两种部署方案中取舍呢？ 从各个方面看，Redis Cluster都是优于主从复制的方案 Redis Cluster能够解决单节点上数据量过大的问题 Redis Cluster能够解决单节点访问压力过大的问题 Redis Cluster包含了主从复制的能力 那是不是代表Redis Cluster永远是优于主从复制的选择呢？ 并不是。 软件架构永远不是越复杂越好，复杂的架构在带来显著好处的同时，一定也会带来相应的弊端。采用Redis Cluster的弊端包括： 维护难度增加。在使用Redis Cluster时，需要维护的Redis实例数倍增，需要监控的主机数量也相应增加，数据备份/持久化的复杂度也会增加。同时在进行分片的增减操作时，还需要进行reshard操作，远比主从模式下增加一个Slave的复杂度要高。 客户端资源消耗增加。当客户端使用连接池时，需要为每一个数据分片维护一个连接池，客户端同时需要保持的连接数成倍增多，加大了客户端本身和操作系统资源的消耗。 性能优化难度增加。你可能需要在多个分片上查看Slow Log和Swap日志才能定位性能问题。 事务和LUA Script的使用成本增加。在Redis Cluster中使用事务和LUA Script特性有严格的限制条件，事务和Script中操作的key必须位于同一个分片上，这就使得在开发时必须对相应场景下涉及的key进行额外的规划和规范要求。如果应用的场景中大量涉及事务和Script的使用，如何在保证这两个功能的正常运作前提下把数据平均分到多个数据分片中就会成为难点。 所以说，在主从复制和集群分片两个方案中做出选择时，应该从应用软件的功能特性、数据和访问量级、未来发展规划等方面综合考虑，只在确实有必要引入数据分片时再使用Redis Cluster。下面是一些建议： 需要在Redis中存储的数据有多大？未来2年内可能发展为多大？这些数据是否都需要长期保存？是否可以使用LRU算法进行非热点数据的淘汰？综合考虑前面几个因素，评估出Redis需要使用的物理内存。 用于部署Redis的主机物理内存有多大？有多少可以分配给Redis使用？对比(1)中的内存需求评估，是否足够用？ Redis面临的并发写压力会有多大？在不使用pipelining时，Redis的写性能可以超过10万次/秒在使用Redis时，是否会使用到pipelining和事务功能？使用的场景多不多？ 综合上面几点考虑，如果单台主机的可用物理内存完全足以支撑对Redis的容量需求，且Redis面临的并发写压力距离Benchmark值还尚有距离，建议采用主从复制的架构，可以省去很多不必要的麻烦。同时，如果应用中大量使用pipelining和事务，也建议尽可能选择主从复制架构，可以减少设计和开发时的复杂度。 Redis Java客户端的选择Redis的Java客户端很多，官方推荐的有三种：Jedis、Redisson和lettuce。 在这里对Jedis和Redisson进行对比介绍 Jedis： 轻量，简洁，便于集成和改造 支持连接池 支持pipelining、事务、LUA Scripting、Redis Sentinel、Redis Cluster 不支持读写分离，需要自己实现 文档差（真的很差，几乎没有……） Redisson： 基于Netty实现，采用非阻塞IO，性能高 支持异步请求 支持连接池 支持pipelining、LUA Scripting、Redis Sentinel、Redis Cluster 不支持事务，官方建议以LUA Scripting代替事务 支持在Redis Cluster架构下使用pipelining 支持读写分离，支持读负载均衡，在主从复制和Redis Cluster架构下都可以使用 内建Tomcat Session Manager，为Tomcat 6/7/8提供了会话共享功能 可以与Spring Session集成，实现基于Redis的会话共享 文档较丰富，有中文文档 对于Jedis和Redisson的选择，同样应遵循前述的原理，尽管Jedis比起Redisson有各种各样的不足，但也应该在需要使用Redisson的高级特性时再选用Redisson，避免造成不必要的程序复杂度提升。 Jedis：github：https://github.com/xetorthio/jedis文档：https://github.com/xetorthio/jedis/wiki Redisson：github：https://github.com/redisson/redisson文档：https://github.com/redisson/redisson/wiki","categories":[{"name":"Code","slug":"Code","permalink":"http://xiaoqinghua.site/categories/Code/"},{"name":"Redis","slug":"Code/Redis","permalink":"http://xiaoqinghua.site/categories/Code/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xiaoqinghua.site/tags/Redis/"}]},{"title":"Redis笔记","slug":"Redis笔记","date":"2017-07-13T15:04:49.000Z","updated":"2018-07-14T09:58:11.140Z","comments":true,"path":"Redis笔记.html","link":"","permalink":"http://xiaoqinghua.site/Redis笔记.html","excerpt":"","text":"Redis 重点笔记1、为什么使用 redis2、使用 redis 容易造成什么问题3、单线程的 redis 为什么这么快4、redis 的数据类型，以及每种数据类型的使用场景5、redis 的过期策略以及内存淘汰机制6、redis 和数据库双写一致性问题7、如何应对缓存穿透和缓存雪崩问题8、如何解决 redis 的并发竞争问题 笔记解析1、为什么使用 redis分析：在项目中使用 redis 主要考虑两点性能和并发。除此之外，redis 还具备做分布式锁等其他功能（// TODO 什么其他功能），但是如果只是为了分布式锁这些其他功能，完全还有其他中间件（如 zookeeper 等）代替，并不是非要用 redis。 性能 和 并发 性能： \b如下图所示，我们在碰到需要执行耗时特别久，且执行结果不频繁变动的 SQL，就特别适合将运行结果放入缓存。这样，后面的请求就去缓存中读取，使得请求能够迅速响应，因此提高了性能。 并发： 如下图所示，在高并发的情况下，所有的请求直接访问数据库，数据库会出现连接异常。这个时候，就需要使用 redis 做一个缓冲操作，让请求先访问到 redis，而不是直接访问数据库。 2、使用 redis 容易造成什么问题分析：诚然 redis 解决了很多问题，提高了系统性能，优化了系统处理并发的能力，但是使用 redis 也会遇到解决上述问题进而所产生的衍生问题。 主要是四个问题 缓存和数据库双写一致性问题 缓存雪崩（大面积的数据崩溃） 缓存击穿 缓存并发竞争问题 下文中给出解决方案。 3、单线程的 redis 为什么这么快分析：这个问题其实是对 redis 内部机制的一个考察。redis 是单线程的！！！！ 为什么快，主要是以下三点 纯内存操作 单线程操作，避免频繁切换上下文 采用非阻塞 I/O 多路复用机制（想到了 netty ） I/O 多路复用：单个线程，通过跟踪每个I/O流的状态，来管理多个I/O流。下图是类比到真实的 redis 线程模型：我们使用redis-client在操作的时候，会产生具有不同事件类型的socket。在服务端，有一段I/O 多路复用的程序，将其置入队列之中（//TODO 可理解为一个线程模型，单线程处理），然后发送到文件事件分派器，依次去队列中取，转发到不同的事件处理器中。redis的这个I/O 多路复用机制，还体现在redis提供了select，epoll，evport，kqueue`等多路复用函数库。 4、redis的数据类型，以及每种数据类型的使用场景分析：这是一个基础问题，配合熟记食用更佳。 有五种数据类型 String 常规set/get操作，value可以是String也可以是数字。一般做一些复杂计算功能的缓存。 hash hash数据类型的value存放的是结构化的对象，比较方便的就是操作其中的某个字段。在做单点登录的时候，就可以用这种结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。 list list数据结构，可以做简单的消息队列的功能。另外一个是，可以利用lrange命令，做基于redis的分页功能（lrange mylist 0 10），性能极佳，用户体验很好。 set set数据结构存放的是一堆不重复的集合。所以可以做全局去重的功能。可以使用交集、并集、差集等操作，可以计算共同喜好，全部喜好，自己独有喜好等功能。 sorted set sorted set多了一个权重参数score，集合中的元素能够按照score进行排列。可以做排行榜应用，取TOP N操作，延时任务，范围查找。 5、 redis的过期策略以及内存淘汰机制分析：假如redis只能存 5G 数据量，写入了 10G 的数据，就会删除 5G 数据，怎么删除，按照什么策略删除。 redis 采用 定期删除+惰性删除策略 为什么不用定时删除策略 定时删除，用一个定时器来负责监视 key，过期则自动删除。虽然内存及时释放，但是十分消耗 CPU 资源。在高并发场景下，CPU 应该将时间应用在处理请求上，而不是删除 key，因此不采用这一种策略。 定期删除+惰性删除是如何工作的？ 定期删除，redis 默认每隔100ms检查是否有过期的key，有过期的key直接删除。需要说明的是，redis不是每隔100ms将所有的key检查一次，而是随机抽取进行检查（如果每隔100ms将所有的key检查一次，redis应该已经爆炸💥了），因此如果只采取定期删除策略，会导致很多过期的key没有被删除。于是，惰性删除派上用场了。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间，那么是否过期了？如果过期此时就会被删除。（感觉就是一波亡羊补牢的操作，不过能这样还是挺 NB 的） 采用定期删除+惰性删除就没其他问题了？？？ 不是的！，如果定期删除没有删除key。然后你也没有去get这个key，也就是说惰性删除没有生效。这样，redis的内存占用会越来越大。此时就应该采用内存淘汰机制。 在reids.conf中有一行配置： 1# maxmemory-policy volatile-lru 这个配置就是内存淘汰策略，一下是几种淘汰策略，解析一下： noeviction：当内存不足以容纳新写入的数据时，新写入操作会报错。这个应该没人用吧😂 allkeys-lru：当内存不足以容纳新写入的数据时，在键空间中，移除最近最少使用的key（least recently used）。推荐使用这一种策略。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。这种情况一般是把 redis 既当缓存，又做持久化存储的时候采用。不推荐 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。依然不推荐 //TODO why volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。不推荐。 如果没有设置expire的key，不满足先决条件prerequisites；那么volatile-lru，volatile-random和volatile-ttl策略的行为和noeviction（不删除）基本一致。 6、redis 和数据库双写一致性问题分析：一致性问题是分布式常见的问题，还可以分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致问题。注意：有一个前提，如果对数据有强一致性要求，数据就不能存放在缓存。我们所做的一切只能保证最终一致性。另外，我们所做的方案从根本上来说，只能降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。 1首先，采取正确的更新策略，先更新数据库，再删除缓存。其次，因为可能存在缓存删除失败的问题，提供一个补偿措施即可，例如利用消息队列。 7、如何应对缓存穿透和缓存雪崩问题分析：这个问题，一版高并发的项目可能会遇到，流量几百万左右。 缓存穿透 有人故意去请求缓存中不存在的数据，导致所有的请求怼到数据库上，从而数据库连接异常。 解决方案： 1、利用互斥锁，缓存失效的时候，先去获得锁，得到锁之后，再去请求数据库。没得到锁，则休眠一端时间重试。 2、采用异步更新策略，无论是key是否取到值，都直接返回。value值中维护一个缓存失效时间，缓存如果过期，就异步起一个线程去读数据库，更新缓存。这个方案，需要做缓存预热（羡慕启动前，先预加载缓存）操作。 3、提供一个能迅速判断请求是否有效的拦截机制。比如，利用布隆过滤器（bloom filter），内部维护一系列合法有效的key。迅速判断出，请求所携带的key是否合法有效。如果不合法，则直接返回。 缓存雪崩 缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都到了数据库上，从而导致数据库连接异常。 解决方案： 1、给缓存的失效时间，加上一个随机值，避免集体失效。 2、使用互斥锁，但是吞吐量明显下降了。 3、使用双缓存，两个缓存，A，B。缓存 A 的失效时间是20分钟，缓存 B 不设失效时间。自己做缓存预热操作。然后细分一下几个小点： 从缓存 A 读数据库，有则直接返回。 A 没有读到数据，直接从 B 读数据，有则直接返回，并异步启动一个更新线程。 更新线程同时更新缓存 A 和缓存 B。 8、如何解决 Redis 的并发竞争问题分析：这个问题大致就是，同时又多个子系统去set一个key。一般很多回答是使用redis的事务机制。但是因为我们的生产环境，一般都是redis集群环境，做了数据分片操作。你一个事务中所有涉及到多个key操作的时候，这个key不一定都存储在同一个redis-server上。因此，redis 的事务机制，有点鸡肋。（// TODO） 如下所示： 1、如果这个key操作，不要求顺序，可以准备一个分布式锁，大家去抢锁，抢到了锁就做set操作即可。 2、如果这个key操作，要求顺序，假设有一个key1，系统 A 需要将key1设置为valueA，系统 B 需要将 key1设置为valueB，系统 C 需要将key1设置为valueC。 我们期望按照key1的value值按照valueA–&gt;valueB–&gt;valueC的顺序变化。这时候我们在写入数据库的时候，就需要保存一个时间戳。假设时间戳如下： 123系统A key1 &#123;valueA 3:00&#125;系统B key1 &#123;valueB 3:05&#125;系统C key1 &#123;valueC 3:10&#125; 那么根据上述，假设是系统 B 先抢到锁，将key1设置为{valueB 3:05}。那么接下来，如果 A 抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那么久不做set操作了。 其他方法，如利用队列，将set方法变成串行访问也可以。 本文出处孤独烟，个人只是一字一字的解读，理解前辈总结的经验。我认为只是大致浏览一篇博文，看一篇公众号整理的干货文章，是没有用的，还是要多理解，多实践，加深记忆，才能深刻，涓涓细流，沁人心脾，强劲壮骨。这种方式只是我个人的学习方式，可能比较笨重，比较慢，但好记性不如烂笔头。这也是为什么写博客的原因。有幸","categories":[{"name":"Code","slug":"Code","permalink":"http://xiaoqinghua.site/categories/Code/"},{"name":"Redis","slug":"Code/Redis","permalink":"http://xiaoqinghua.site/categories/Code/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://xiaoqinghua.site/tags/Redis/"}]},{"title":"白夜行","slug":"白夜行","date":"2016-11-14T12:06:59.000Z","updated":"2018-07-21T08:28:46.954Z","comments":true,"path":"白夜行.html","link":"","permalink":"http://xiaoqinghua.site/白夜行.html","excerpt":"","text":"白夜行世界上有两样东西不能直视，一是太阳，二是人心。 我的天空里没有太阳，总是黑夜，但并不暗，因为有东西代替了太阳。虽然没有太阳那么明亮，但对我来说已经足够。凭借这份光，我便能把黑夜当成白天。我从来就没有太阳，所以不怕失去。 曾经拥有的东西被夺走，并不代表就会回到原来没有那种东西的时候。 一天中，太阳会升起，同时还会落下。人生也一样，有白天和黑夜，只是不会像太阳那样，有定时的日出和日落。有些人一辈子都活在太阳的照耀下，有些人不得不一直活在漆黑的深夜里。人害怕的，就是本来一直存在的太阳落下不再升起，也就是非常害怕原本照在自己身上的光芒消失。 一个把平等置于自由智商的社会两者都得不到。一个把自由置于平等之上的社会，很大程度上可以两者兼得。 只有在一个方面，联邦储备体系始终如一。这边是，他把所有问题都归咎于超出自己控制能力的外部影响，而把所有合意的结果都归功于自己。由此他继续维持着那个谬传，说私人经济是不稳定的；而他的所作所为却不断证明了这一事实即政府才是导致今天经济不稳定的主要根源。 公众和经济学家观念的改变，均源自对实际情况的误解。当时只有少数人知道，而我们现在都知道，大萧条并非是私有制企业失败所导致而是因为政府并未成功履行它被赋予的责任。这些责任用《合众国宪法》第一天第八款的话来说，便是“铸造货币，调节其价值，并厘定外币价值”。不幸的是，在第九章我们将看到，政府在管理货币方面的失败不仅是历史上的一桩怪事，而且任是今日之事实。 没有什么比时间更有说服力了，因为时间无需通知我们就可以改变一切。 最初我们来到这个世界上是因为不得不来的。最终我们离开这个世界，是因为我们不得不走。 以笑的方式哭，在死亡的伴随下活着。 作为一个词语，“活着”在我们中国的语言里充满了力量，它的力量不是来自于喊叫，也不是来自于进攻，而是忍受，去忍受生命赋予我们的责任，去忍受现实给我们的幸福和苦难，无聊和平庸。 做人不能忘记四条，话不要说错，床不要睡错，门槛不要踏错，口袋不要摸错。 人是为活着本身而活着，不是为了活着之外的任何事物所活着。","categories":[],"tags":[{"name":"Life","slug":"Life","permalink":"http://xiaoqinghua.site/tags/Life/"}]},{"title":"影响力","slug":"影响力","date":"2016-10-27T13:35:02.000Z","updated":"2018-07-14T13:30:46.906Z","comments":true,"path":"影响力.html","link":"","permalink":"http://xiaoqinghua.site/影响力.html","excerpt":"","text":"影响力越喜欢一个人，受这个人的影响越大。 一样原本没有什么吸引力的东西，突然间变得很有诱惑力，就因为很快你就得不到它了。 在接受了别人的好处之后，人们很亲依就会答应一个在没有负债心里时一定会拒绝的请求。即便是一个不请自来的好处，一旦被接收，也会制造出一种负债感。 当我们对自己缺乏信心时，当形式变得不很明朗时，当不确定占了上风时，我们最有可能以别人的行为作为自己行动的参照。与我们类似的人的行为对我们最有影响力。 一旦我们做了一个决定，或选择了一种立场，就会有发自内心以及来自外部的压力来迫使我们与此保持一致。在这种压力下，我们总是希望以实际行动来证明我们以前的决定是正确的。 cazhulafu brother上帝与魔鬼在那里搏斗，战斗便在人们心中。 在现实主义者身上，并不是奇迹产生信仰，而是信仰产生奇迹。 在大多数情况下，人们，甚至恶人，要比我们想象中的他们幼稚的多，天真的多。其实我们也一样。 对自己说谎和听自己说谎的人会落到这样的地步：无论在自己身上还是周围，即使有真理，他也无法辨别，结果将是既不自重，也不尊重别人。一个人如果对谁都不尊重，也就没有了爱；在没有爱的情况下想要消遣取乐，无非放纵情欲，耽于原始的感官享受，在罪恶的泥潭中完全堕落成畜类，而一切都始于不断的对人和对己说谎。对自己说谎的人最容易怄气。 电子游戏让人上瘾的全部原因只有一个。创造一个属于我们自己的世界。我想不出有什么东西比做一个神更能让人上瘾的了。 人们都说没有什么能逃脱冷库的热力学第二定律，宇宙的最后归宿是一片热死寂。但这不是故事的全部，宇宙在沉寂的同事，也在热闹起来，从旧物中带来新生、增加复杂性的新层次。宇宙充满了无尽的创造力。熵和进化，两者就像两支时间之矢，一头拖着我们退入无穷的黑暗，一头在拉扯我们走向永恒的光明。 网络把隐私从道德领域转移到了市场领域——隐私成了一种商品。","categories":[],"tags":[{"name":"Life","slug":"Life","permalink":"http://xiaoqinghua.site/tags/Life/"}]},{"title":"少有人走的路","slug":"少有人走的路","date":"2016-09-27T13:39:24.000Z","updated":"2018-07-14T16:24:50.691Z","comments":true,"path":"少有人走的路.html","link":"","permalink":"http://xiaoqinghua.site/少有人走的路.html","excerpt":"","text":"少有人走的路一辈子真的很短，远没有我们想象的那么长，永远真的没有多远，所有不妨对爱你的好一点，也对自己好一点，今天是你的枕畔人，明天就可能是你的陌生人，如果这一辈子来不及好好爱，就更不要指望下辈子还能遇见。 想让别人真正爱你，只有让自己成为值得爱的人。 承受痛苦是走向成熟的必经之路，任何人都不能回避。 大部分的恐惧与懒惰有关，这句话我深以为然。我么常常会害怕改变，其实都是因为自己太懒了，懒得去适应新环境，懒得去学习新知识，涉足新的领域，但如果总是这样的话，如何能让自己成熟起来呢？ 多数人认为勇气就是不害怕。现在让我来告诉你，不害怕不是勇气，它只是脑损伤。勇气是尽管你感觉害怕，但仍能迎难而上；尽管你感觉痛苦，但仍能直接面对。 所谓自律，是以积极而主动地态度，去解决人生痛苦的重要原则，主要包括四个方面：推迟满足感、承担责任、尊重事实、保持平衡。 未来简史更重要的\b，当以大数据、人工智能为代表的科学技术发展的日益成熟，人类将面临着从进化到智人以来最大的一次改变，绝大部分人将沦为“无价值的群体”，只有少部分人能进化成特质发生改变的”神人”。 未来，人类将面临\b着三大问题：生物本身就是算法，生命是不断处理数据的过程；意识与智能的分离；拥有大数据积累的外部环境将比我们自己更了解自己。如何看待这三大问题，以及如何采取应对措施，将直接影响着人类未来的发展。 天才在左，疯子在右时间是不流逝的，流逝的是我们。 哲学家与疯子的区别，一个知识在想，另一个真的去做了。 孤独感时常体现在一种矛盾上，就是你经常处于一种挣扎状态：既希望别人关心、管住自己，又不知道该怎么去接触和回应别人。于是干脆直接抗拒，科室骨子里又是那么的渴望被了解和关注，而且矛盾到嘴里说出来的和心里想的完全相反。 只有当你认真的去做一件事情的时候，才会发现自己的灵魂和灵魂深处。 如果有一天你看到我疯了，其实是你疯了。","categories":[],"tags":[{"name":"Life","slug":"Life","permalink":"http://xiaoqinghua.site/tags/Life/"}]},{"title":"乌合之众","slug":"乌合之众","date":"2016-08-08T12:23:54.000Z","updated":"2018-07-14T13:58:28.482Z","comments":true,"path":"乌合之众.html","link":"","permalink":"http://xiaoqinghua.site/乌合之众.html","excerpt":"","text":"乌合之众人一到群体中，智商就严重降低，为了获得认同感，个体原意抛弃是非，用智商去换取那份让人倍感安全的归属感。 群体不善推理，却又急于行动。 我们始终有一种错觉，以为我们的感情源自我们的内心。 我们以为自己是理性的我们以为自己的一举一动都是有其道理的。但事实上，我们绝大多数的日常行为，都是一些我们自己根本无法了解的隐蔽动机结果。 个人一旦成为群体中的一员，他所作所为就不会再承担责任，这是每个人都会暴露出自己不受约束的一面。群体追求和相信的从来不是什么真相和理性，而是盲从、残忍、偏执和狂热，只知道简单而极端的感情。 所谓信仰，它让一个人变得完全受自己的梦想奴役。 月亮与六便士上帝的磨盘转的很慢，却磨得很细。 在爱情的事上，如果你考虑起自尊心来，那只能有一个原因：实际上你还是最爱你自己。 我们每个人生在世界上都是孤独的。每个人都困在一座铁塔里，只能靠一些符号同别人传达自己的思想。而这些符号并没有共同的价值，因此他们的意义是模糊的、不确定的。我嗯非常可怜地想把自己心中的财富传送给别人，但他们却没有接受这财富的能力。因此我们只能孤独地行走，尽管身体相互依傍却并不在一起，既不了解别人也不能被别人所了解。 海伯利安看了这本书，你就不会再看《三体》了。 我去旅行是因为我决定要去，并不是因为风景。 趁年轻，好好利用这个机会，尽力去尝遍所有的痛苦，这种事可不是一辈子什么时候都可以遇到的。 人不是从娘胎里出来就一成不变的，相反，生活会逼他一次又一次的脱胎换骨。 真正的爱情需要什么？需要两个人在一起是轻松快乐的，没有压力的。 诚实的生活方式其实是按照自己身体的意愿行事，饿的时候吃饭，爱的时候不必撒谎。 苏菲的世界生命本来就是悲伤而严肃的。我们来到这个美丽的世界里，彼此相逢，彼此问候，并结伴同游一段短暂的时间。然后我们就是去了对方，然后就莫名其妙的消失了，就像我们莫名其妙地来到世界一般。 你太习惯这个世界了，所以你对任何事情都不感到惊奇。 真正的知识来自内心，而不是得自别人的传授。同时唯有出自内心的知识，才能使人拥有真正的智慧。 理智和经验都缺失时，就会出现一个真空，这个真空可以有信仰填补。","categories":[],"tags":[{"name":"Life","slug":"Life","permalink":"http://xiaoqinghua.site/tags/Life/"}]},{"title":"娱乐至死","slug":"娱乐至死","date":"2016-07-03T05:26:41.000Z","updated":"2018-07-14T16:24:19.207Z","comments":true,"path":"娱乐至死.html","link":"","permalink":"http://xiaoqinghua.site/娱乐至死.html","excerpt":"","text":"娱乐至死有两种方法可以让文化精神枯萎，一种是奥威尔式——文化成为一种监狱，另一种是赫胥黎式——文化成为一场滑稽戏。 我们将死于我们所热爱的东西。 \b真理不能，也从来没有，毫无修饰地存在。它必须穿着\b某种合适的外衣出现，否则就可能得不到承认，这也正说明了“真理”是一种文化偏见。 过去，人们为了解决生活中的问题儿搜索信息，现在是为了让无用的\b信息派上用场而制造问题。 一个人学到的\b最重要的东西是学习的方法。 1984他们不到觉悟的时候，就不会造反。他们不造反，就不会有觉悟。 如果你感到保持人性是值得的，即使这不能有任何结果，你也已经打败了他们。 上等人的目标是保持他们的地位。中等人的目标是要通上等人交换地位。下等人的特点始终是，他们劳苦之余无暇旁顾，偶尔才顾到日常生活以外的事，因此如果他们有目标的话，无非就是取消一切差别，建立一个人人平等的社会。 我们很明白没有人会为了废除权利而夺取权利。权力不是手段，权力是目的。建立专政不是为了保卫革命，反过来进行革命是为了建立专政。 zoo越喜欢上什么，一旦突然失去它，我的心就越为悲泣。这样反反复复地，我还任要忍耐这种苦楚不得不继续度过剩余的生命。这是多么残酷的事情啊。与其如此，不如干脆把我当做什么都不爱，没有人心徒有人形的样子。 爱的越深刻，死的意义就越沉重，失落感也会越深刻。 我脸上越是挤出明亮的笑容，我的内心就越是变的荒芜。 我对别人的存在感到恐惧，我觉得自己对别人谄媚的行径也会因为这个。被别人讨厌，被别人看不起，被别人嘲笑，对我来说都是难以忍受的痛苦，于是为了逃避这些行为，我再自己的内心养了那样一只丑陋的动物。如果没有别人在这个世界上，只有我自己一个人，那将是多么轻松啊。 ps:这本书太沉重了吧😨，配个阳光明媚的 landspace 吧。","categories":[],"tags":[{"name":"Life","slug":"Life","permalink":"http://xiaoqinghua.site/tags/Life/"}]},{"title":"百年孤独","slug":"百年孤独","date":"2016-06-27T09:26:41.000Z","updated":"2018-07-14T09:57:42.023Z","comments":true,"path":"百年孤独.html","link":"","permalink":"http://xiaoqinghua.site/百年孤独.html","excerpt":"","text":"百年孤独无论走到哪里，都应该记住，过往的一切都是假的，回忆是一条没有尽头的路，一切以往的春天都不复存在，就连那最坚韧\b而又狂乱的爱情归根结底也不过是一种转瞬即逝的现实，唯有孤独永恒。 生命中真正重要的不是你遭遇了什么，而是你记住了哪些事，又是如何铭记的。 我们趋行在人生这个亘古的旅途，在坎坷中奔跑，在挫折中涅槃，忧愁缠满全身，痛苦飘洒一地。我们累，却无从止歇；我们苦，却无法回避。 买下一张永久车票，登上一辆永无终点的列车。 所有人都显得很激进，用自己的方式想尽办法派遣寂寞，事实上仍是延续自己的孤独。寂寞是造化对群居者的诅咒，孤独才是寂寞的唯一出口。","categories":[],"tags":[{"name":"Life","slug":"Life","permalink":"http://xiaoqinghua.site/tags/Life/"}]}]}